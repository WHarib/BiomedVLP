# app.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
BioViL-T multimodal embedding API (CPU)
--------------------------------------
Routes
  â€¢ POST /embed_text   â†’ 512-d text embedding
  â€¢ POST /embed_image  â†’ 512-d image embedding
  â€¢ POST /similarity   â†’ cosine(image â†”ï¸Ž text)
"""

import subprocess, sys, io, torch
from pathlib import Path
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from transformers import AutoTokenizer, AutoModel
from PIL import Image

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_ID   = "microsoft/BiomedVLP-BioViL-T"
IMG_CKPT   = "biovil_t_image_model_proj_size_128.pt"  # lives in the repo
DEVICE     = torch.device("cpu")                      # Spaces default

# install hi-ml-multimodal on first cold-start (â‰ˆ 5-10 s)
try:
    import health_multimodal                            # noqa: F401
except ImportError:                                     # pragma: no cover
    subprocess.check_call(
        [sys.executable, "-m", "pip", "install", "-q",
         "hi-ml-multimodal>=0.2.1", "pydicom", "opencv-python-headless"]
    )

from health_multimodal.image import get_biovil_resnet_inference
from health_multimodal.vlp   import ImageTextInferenceEngine

# â”€â”€ Text encoder (CXRBERT) via ðŸ¤— Transformers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
txt_model = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True).eval()

def _text_embedding(text: str) -> torch.Tensor:
    toks = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        return txt_model.get_projected_text_embeddings(**toks).squeeze(0)  # (512,)

# â”€â”€ Image encoder via hi-ml (loads ViT-ResNet + projector) â”€â”€â”€â”€â”€â”€â”€â”€â”€
ckpt_path = Path(__file__).with_name(IMG_CKPT)           # auto-downloaded by HF
img_infer = get_biovil_resnet_inference(str(ckpt_path), device=DEVICE)

def _image_embedding(pil_img: Image.Image) -> torch.Tensor:
    with torch.no_grad():
        return img_infer.get_projected_global_embedding(pil_img).squeeze(0)  # (512,)

# joint engine (not strictly required but handy if you expand later)
joint_engine = ImageTextInferenceEngine(
    image_inference_engine=img_infer,
    text_inference_engine=None  # we handle text ourselves
)

# â”€â”€ FastAPI app & landing page â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(docs_url="/docs")

@app.get("/", response_class=HTMLResponse)
async def index() -> str:  # noqa: D401
    return (
        "<h2>BioViL-T multimodal embedding API (CPU)</h2>"
        "<ul>"
        "<li><code>POST /embed_text</code> â€“ text â†’ 512-d vector</li>"
        "<li><code>POST /embed_image</code> â€“ image â†’ 512-d vector</li>"
        "<li><code>POST /similarity</code> â€“ image + text â†’ cosine score</li>"
        "</ul>"
    )

# â”€â”€ /embed_text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post("/embed_text")
async def embed_text(text: str = Form(...)):
    text = text.strip()
    if not text:
        raise HTTPException(400, "Empty text prompt.")
    vec = _text_embedding(text)
    return JSONResponse({"embedding": vec.tolist()})

# â”€â”€ /embed_image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post("/embed_image")
async def embed_image(file: UploadFile = File(...)):
    try:
        pil = Image.open(io.BytesIO(await file.read())).convert("RGB")
    except Exception as e:
        raise HTTPException(400, f"Bad image: {e}")
    vec = _image_embedding(pil)
    return JSONResponse({"embedding": vec.tolist()})

# â”€â”€ /similarity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post("/similarity")
async def similarity(file: UploadFile = File(...), text: str = Form(...)):
    pil      = Image.open(io.BytesIO(await file.read())).convert("RGB")
    img_vec  = _image_embedding(pil)
    text_vec = _text_embedding(text.strip())
    score    = torch.nn.functional.cosine_similarity(img_vec, text_vec, dim=0).item()
    return JSONResponse({"cosine_similarity": score})
